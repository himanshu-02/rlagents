{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577caf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn                        #To handle layers.\n",
    "import torch.nn.functional as F              #For value activation function.\n",
    "import torch.optim as optim                  #For adam optimizer\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e403f4",
   "metadata": {},
   "source": [
    "This implementation uses an Agent that has a Deep Q-Learning network to train the LunarLander-v2 enviornment.\n",
    "Uses linear layers as the enviornment is an eight element vector observation \n",
    "\\[Horizontal Coordinate, Vertical Coordinate, Horizontal Speed, Vertical Speed, Angle, Angular Speed, 1 if first leg has contact else 0, 1 if second leg has contact, else 0]\\\n",
    "\n",
    "The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector.\n",
    "Reward for moving from the top of the screen to the landing pad and zero speed is about 100..140 points.\n",
    "If the lander moves away from the landing pad it loses reward. The episode finishes if the lander crashes or\n",
    "comes to rest, receiving an additional -100 or +100 points. Each leg with ground contact is +10 points.\n",
    "Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame.\n",
    "Solved is 200 points.\n",
    "Landing outside the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land\n",
    "on its first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()              #Mean squared error loss\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "         \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions, max_mem_size=1000000, eps_end=0.01, eps_dec=5e-4):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        self.Q_eval = DeepQNetwork(self.lr, n_actions=n_actions, input_dims=input_dims, fc1_dims=256, fc2_dims=256)\n",
    "                                                                                            #Evaluation network\n",
    "        \n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "                                                                                #Storing state memory as named array \n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "                                                                                #Storing new state memory as named array\n",
    "        #TD update rule requires the value of current state, next state; and the reward it recieved\n",
    "        \n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done \n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "        \n",
    "    def choose_action(self, observation):                                        #choose action using epsilon \n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return                                      #If atleast the batch size of memory is not filled, do actions at random\n",
    "        \n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
    "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
    "        reward_batch= T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
    "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
    "                                                        #Converting np array subset of memory into pytorch tensor.\n",
    "        \n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        \n",
    "        q_target = reward_batch + self.gamma * T.max(q_next, dim=1)[0] \n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
    "                        else self.eps_min\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=4,\n",
    "                     eps_end=0.01, input_dims=[8], lr=0.001)\n",
    "    scores, eps_history = [], []\n",
    "    n_games = 500\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            if i >= (n_games - 5) or i<5:\n",
    "                env.render()\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action, reward, \n",
    "                                  observation_, done)\n",
    "            agent.learn()\n",
    "            observation = observation_\n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "        \n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        \n",
    "        print('episode ', i, 'score %.2f' % score, \n",
    "                  'average score %.2f' % avg_score,\n",
    "                  'epsilon %.2f' % agent.epsilon)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32eac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            observation = observation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
